{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Notebook\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrick-batman/Mosaic-24/blob/main/Mosaic%20PS2/char_level_rnn.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task can be modeled as masked language modelling whereby we need to predict the next the next character; so one of the approaches can be to use character level masked language model:\n",
    "\n",
    "- <b>RNN</b>\n",
    "- <b>LSTM</b>\n",
    "- <b>Transformer</b> (Encoder only)\n",
    "<br />\n",
    "\n",
    "\n",
    "\n",
    "However since the game requires us only to predict one output at a time rather than fill all the character masks at once, we fill the letter with maximum probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take batches of words, pass them through simple RNN; use last output to predict probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MaskedLanguageModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now while making dataset; I have implement `soft-cross entropy loss`; that is suppose you have word '___le' to be modelled from 'apple' now since here 'a' as well as 'p' both a missing we assing weights to both of them s/t it is proportional to their missing frequency hoping that this will enable our model to capture language syntactics as well. \n",
    "- `a`: 0.33\n",
    "- `p`: 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, masked_words, original_words, char_to_index):\n",
    "        self.masked_words = masked_words\n",
    "        self.original_words = original_words\n",
    "        self.char_to_index = char_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.masked_words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masked_word = self.masked_words[idx]\n",
    "        original_word = self.original_words[idx]\n",
    "        \n",
    "        # Convert characters to indices\n",
    "        masked_indices = [self.char_to_index[c] for c in masked_word]\n",
    "        \n",
    "        # Convert original word to soft encoding\n",
    "        weights = [0]*len(self.char_to_index)\n",
    "        total_diff = 0\n",
    "        for i in range(len(original_word)):\n",
    "            if original_word[i] != masked_word[i]:\n",
    "                weights[self.char_to_index[original_word[i]]] += 1 \n",
    "                total_diff += 1\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = weights[i]/total_diff\n",
    "        \n",
    "        original_indices = torch.tensor(weights)\n",
    "\n",
    "        \n",
    "        return torch.tensor(masked_indices), original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with batches with different lengths of individual words; we need to pad them so that every tensor in a batch can be stacked. For this we have `collate_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    masked_words, original_words = zip(*batch)\n",
    "    max_len = max(len(word) for word in masked_words)\n",
    "    padded_masked_words = torch.stack([torch.nn.functional.pad(word, (0, max_len - len(word)), value=0) for word in masked_words])\n",
    "    stacked_orignal_words = torch.stack(original_words)\n",
    "    # padded_original_words = torch.stack([torch.nn.functional.pad(word, (0, max_len - len(word)), value=0) for word in original_words])\n",
    "    return padded_masked_words, stacked_orignal_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gool-ol' training loop.\n",
    "\n",
    "\n",
    "\n",
    "PS:This is going to perform extremely poorly since its trained on few words and only for 2 epochs but this will help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for masked_words, weights_chars in tqdm(dataloader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(masked_words)\n",
    "            loss = criterion(output, weights_chars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just to give an example I have just created a list of how you would want to sample the words for language(character) modelling.\n",
    "Ideally you would want to use 70-80% training set for this and leave rest for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "masked_words = ['a__l_', 'p__', 'l__r___g', 'bott_e']\n",
    "original_words = ['apple', 'pen', 'learning', 'bottle']\n",
    "\n",
    "## storing how words are mapped so that we can convert them back again\n",
    "char_to_index = {chr(i): i - 96 for i in range(97, 123)}\n",
    "char_to_index.update({'_': 27})\n",
    "char_to_index.update({'-': 0})\n",
    "index_to_char = {i: char for char, i in char_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 2/2 [00:00<00:00, 23.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 3.333022952079773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 2/2 [00:00<00:00, 745.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 3.2866896390914917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = len(char_to_index)\n",
    "hidden_size = 128\n",
    "batch_size = 3\n",
    "epochs = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "model = MaskedLanguageModel(input_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = CustomDataset(masked_words, original_words, char_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test(model, dataloader, char_to_index, index_to_char):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for masked_words, original_words in dataloader:\n",
    "            output = model(masked_words)\n",
    "            char_ind = [np.argmax(output[i].detach().numpy()) for i in range(output.shape[0])]\n",
    "            predictions = [index_to_char[index] for index in char_ind]\n",
    "            print(predictions)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'n']\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "masked_words_test = ['c__k', 'b_t']\n",
    "original_words_test = ['cook', 'bat']\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = CustomDataset(masked_words_test, original_words_test, char_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "# Test the model\n",
    "test(model, dataloader, char_to_index, index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase you need to unload the model uncomment the below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from the .pth file\n",
    "# loaded_model = MaskedLanguageModel(input_size, hidden_size)\n",
    "# loaded_model.load_state_dict(torch.load('model.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
